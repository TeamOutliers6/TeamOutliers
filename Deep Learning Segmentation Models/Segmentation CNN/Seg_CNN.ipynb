{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegmentationCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01SQAqdSYrZq",
        "outputId": "3bc99a9d-3bca-4dbf-dd1d-fa4ebb7051b1"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-1ZlBchZU1N",
        "outputId": "3eb3cc9b-436b-46b0-93a2-629beaeedbc6"
      },
      "source": [
        "%cd /content/gdrive/My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvK5yNURHzOI"
      },
      "source": [
        "In the following step we are using cleaned images as inputs and painted images generated from ImageJ as masks. We Initially used thresholded images as masks but we could not get satisfactory results. You can observe the results for thresholded images by changing the directory from painted images to thresholded images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMzsRgqZ-GnT"
      },
      "source": [
        "#importing required packages\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import glob\n",
        "import skimage.io as io\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imread, imshow\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "my_epochs = 20 #setting epochs on network to be 20\n",
        "img_dims = 256 #image size\n",
        "batch_size = 32 #batchsize for the network\n",
        "X_train = []\n",
        "Y_train = []\n",
        "#accessing all the cleaned images which we use as inputs and painted images generated from ImageJ tool as masks.\n",
        "i_path = 'sarcopenia-ai-master/Deblurred_Outputs/*.jpg'\n",
        "m_path = 'sarcopenia-ai-master/deOutputs/*.jpg'\n",
        "for filename in glob.glob(i_path):\n",
        "  img = cv2.imread(filename)\n",
        "  train_imgs = resize(img, (img_dims,img_dims, 3), mode='constant', preserve_range=True)\n",
        "  X_train.append(list(train_imgs))\n",
        "for filename in glob.glob(m_path):\n",
        "  img = cv2.imread(filename)\n",
        "  mask_imgs = resize(img, (img_dims, img_dims, 1), mode='constant', preserve_range=True)\n",
        "  Y_train.append(list(mask_imgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma7V6loucNsN"
      },
      "source": [
        "#importing required packages\n",
        "import tensorflow as tf\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "#s)\n",
        "#defining inputs for the network\n",
        "inputs = tf.keras.layers.Input((img_dims, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4QjNXPDI5Gq"
      },
      "source": [
        "For this segmentation CNN approach we followed Yunzhi wang and team approach where they used combination of CONV2D, Maxpooling and Upsampling layers. But we were initially getting shape errors with Conv2D so we proceeded with Conv1D. Here also max pooling and upsampling were not working. So we just used five Conv1D layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V345ZwtcUd1"
      },
      "source": [
        "\n",
        "c1 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "c1 = tf.keras.layers.Dropout(rate=0.2)(c1)\n",
        "c1 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(c1)\n",
        "# c1 = tf.keras.layers.MaxPooling1D(pool_size=2)(c1)\n",
        "\n",
        "\n",
        "# #second convolutional block\n",
        "c2 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(c1)\n",
        "c2 = tf.keras.layers.Dropout(rate=0.2)(c2)\n",
        "c2 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(c2)\n",
        "# # c2 = tf.keras.layers.MaxPooling1D(pool_size=2)(c2)\n",
        "\n",
        "# # # #third convolutional block\n",
        "c3 = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(c2)\n",
        "c3 = tf.keras.layers.Dropout(rate=0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(c3)\n",
        "# # c3 = tf.keras.layers.MaxPooling1D(pool_size=2)(c3)\n",
        "\n",
        "# # # #Upsampling and fourth convolutional block\n",
        "# # u1 = tf.keras.layers.UpSampling1D(size=2)(c3) \n",
        "# # u1 = tf.keras.layers.concatenate([u1, c1], axis=-1)\n",
        "c4 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(c3)\n",
        "c4 = tf.keras.layers.Dropout(rate=0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(c4)\n",
        "\n",
        "# # # # c4 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(c3)\n",
        "# # # # c4 = tf.keras.layers.Dropout(rate=0.2)(c4)\n",
        "# # # # c4 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(c4)\n",
        "# # # # c4 = tf.keras.layers.MaxPooling1D(pool_size=2)(c4)\n",
        "# # # # #Upsampling and fifth convolutional block\n",
        "# # u2 = tf.keras.layers.UpSampling1D(size=2)(c4) \n",
        "# # # u2 = tf.keras.layers.concatenate([u2, c1], axis=-1)\n",
        "c5 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(c4)\n",
        "c5 = tf.keras.layers.Dropout(rate=0.2)(c5)\n",
        "c5 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(c5)\n",
        "# # c5 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(c4)\n",
        "# # c5 = tf.keras.layers.Dropout(rate=0.2)(c5)\n",
        "# # c5 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(c5)\n",
        "# # c5 = tf.keras.layers.MaxPooling1D(pool_size=2)(c5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUkJ95AMJrdF"
      },
      "source": [
        "The follwing commented code is Conv1D with Maxpooling and Upsampling. You can uncomment and observe the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaCNYKubPK6U"
      },
      "source": [
        "# c1 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same')(inputs)\n",
        "# c1 = tf.keras.layers.Dropout(rate=0.2)(c1)\n",
        "# c1 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same')(c1)\n",
        "# c1 = tf.keras.layers.MaxPooling1D(pool_size=(2))(c1)\n",
        "\n",
        "# #second convolutional block\n",
        "# c2 = tf.keras.layers.Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same')(c1)\n",
        "# c2 = tf.keras.layers.Dropout(rate=0.2)(c2)\n",
        "# c2 = tf.keras.layers.Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same')(c2)\n",
        "# c2 = tf.keras.layers.MaxPooling1D(pool_size=(2))(c2)\n",
        "\n",
        "# #third convolutional block\n",
        "# c3 = tf.keras.layers.Conv1D(filters=128, kernel_size=(3), activation='relu', padding='same')(c2)\n",
        "# c3 = tf.keras.layers.Dropout(rate=0.2)(c3)\n",
        "# c3 = tf.keras.layers.Conv1D(filters=128, kernel_size=(3), activation='relu', padding='same')(c3)\n",
        "# c3 = tf.keras.layers.MaxPooling1D(pool_size=(2))(c3)\n",
        "\n",
        "\n",
        "# cf = tf.keras.layers.Conv1D(filters = 256, kernel_size=(3), activation='relu', padding='same')(c3)\n",
        "# cf = tf.keras.layers.Dropout(rate=0.2)(cf)\n",
        "# cf = tf.keras.layers.Conv1D(filters= 256, kernel_size=(3), activation='relu', padding='same')(cf)\n",
        "\n",
        "# cf = tf.keras.layers.MaxPooling1D(pool_size=(2))(cf) \n",
        "\n",
        "# #Upsampling and fourth convolutional block\n",
        "# u1 = tf.keras.layers.UpSampling1D(size=(2))(cf) \n",
        "# u1 = tf.keras.layers.concatenate([c3, u1], axis=-1)\n",
        "# c4 = tf.keras.layers.Conv1D(filters=128, kernel_size=(3), activation='relu', padding='same')(u1)\n",
        "# c4 = tf.keras.layers.Dropout(rate=0.2)(c4)\n",
        "# c4 = tf.keras.layers.Conv1D(filters=128, kernel_size=(3), activation='relu', padding='same')(c4)\n",
        "\n",
        "# #Upsampling and fifth convolutional block\n",
        "# u2 = tf.keras.layers.UpSampling1D(size=(2))(c4) \n",
        "# u2 = tf.keras.layers.concatenate([u2, c2], axis=-1)\n",
        "# c5 = tf.keras.layers.Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same')(u2)\n",
        "# c5 = tf.keras.layers.Dropout(rate=0.2)(c5)\n",
        "# c5 = tf.keras.layers.Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same')(c5)\n",
        "# #Upsampling and sixth convolutional block\n",
        "# u3 = tf.keras.layers.UpSampling1D(size=(2))(c5) \n",
        "# u3 = tf.keras.layers.concatenate([u3, c1], axis=-1)\n",
        "# c6 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same')(u3)\n",
        "# c6 = tf.keras.layers.Dropout(rate=0.2)(c6)\n",
        "# c6 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same')(c6)\n",
        "\n",
        "# uf = tf.keras.layers.UpSampling1D(size=(2))(c6) \n",
        "# # uf = tf.keras.layers.concatenate([uf, c1], axis=-1)\n",
        "# c7 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same')(uf)\n",
        "# c7 = tf.keras.layers.Dropout(rate=0.2)(c7)\n",
        "# c7 = tf.keras.layers.Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same')(c7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX65dcnPKGDR"
      },
      "source": [
        "The following commented code is our attempt for Conv2D. Conv2D did work when we changed the dimensions of the image. \n",
        "In this code:  train_imgs = resize(img, (img_dims,img_dims, 3) add one more channel and uncomment the follwing code to see how Conv2D works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvlyJX26EABM"
      },
      "source": [
        "# c1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "# c1 = tf.keras.layers.Dropout(rate=0.2)(c1)\n",
        "# c1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(c1)\n",
        "# c1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1)\n",
        "\n",
        "# #second convolutional block\n",
        "# c2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(c1)\n",
        "# c2 = tf.keras.layers.Dropout(rate=0.2)(c2)\n",
        "# c2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(c2)\n",
        "# c2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2)\n",
        "\n",
        "# #third convolutional block\n",
        "# c3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(c2)\n",
        "# c2 = tf.keras.layers.Dropout(rate=0.2)(c3)\n",
        "# c3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(c3)\n",
        "# c3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3)\n",
        "\n",
        "# #Upsampling and fourth convolutional block\n",
        "# u1 = tf.keras.layers.UpSampling2D(size=(2,2))(c3) \n",
        "# u1 = tf.keras.layers.concatenate([u1, c2], axis=-1)\n",
        "# c4 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(u1)\n",
        "# c4 = tf.keras.layers.Dropout(rate=0.2)(c4)\n",
        "# c4 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(c4)\n",
        "\n",
        "# #Upsampling and fifth convolutional block\n",
        "# u2 = tf.keras.layers.UpSampling2D(size=(2,2))(c4) \n",
        "# u2 = tf.keras.layers.concatenate([u2, c1], axis=-1)\n",
        "# c5 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(u2)\n",
        "# c5 = tf.keras.layers.Dropout(rate=0.2)(c5)\n",
        "# c5 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(c5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS-v7sAycdT2"
      },
      "source": [
        "# defining outputs for network. The best result was produced for c1\n",
        "outputs = tf.keras.layers.Conv1D(filters=1, kernel_size = 1, activation='sigmoid')(c1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06IXqGDqeeco",
        "outputId": "6173c51e-828b-40fe-fcd5-7b50769ba5e5"
      },
      "source": [
        "# Creating model and compiling\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "model = tf.keras.Model(inputs, outputs )\n",
        "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "# model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), optimizer=optimizer, metrics=['accuracy'])\n",
        "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "# checkpoint = ModelCheckpoint(filepath='xray_model.hdf5', save_best_only=True, save_weights_only=True)\n",
        "# lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
        "checkpoint = ModelCheckpoint(filepath='xray_model.hdf5', save_best_only=True, save_weights_only=True)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, mode='min')\n",
        "board = TensorBoard(log_dir='logs')\n",
        "\n",
        "my_callbacks = [\n",
        "    checkpoint,\n",
        "    lr_reduce,\n",
        "    early_stop,\n",
        "    board\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 3)]          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 256, 32)           320       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 256, 32)           3104      \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 256, 1)            33        \n",
            "=================================================================\n",
            "Total params: 3,457\n",
            "Trainable params: 3,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osHLMaEce7-8",
        "outputId": "e79ff00e-b10b-4dc8-862c-405b7f9188df"
      },
      "source": [
        "#Splitting data into training and testing sets and fitting the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# dim=512\n",
        "# images = np.array(images).reshape(len(images),dim,dim,1)\n",
        "# masks = np.array(masks).reshape(len(masks),dim,dim,1)\n",
        "\n",
        "train_images, validation_images, train_masks, validation_masks = train_test_split((train_imgs-127.0)/127.0, \n",
        "                                                            (mask_imgs>127).astype(np.float32), \n",
        "                                                            test_size = 0.1,random_state = 2018)\n",
        "\n",
        "train_images, test_images, train_masks, test_masks = train_test_split(train_images,train_masks,\n",
        "                                                            test_size = 0.1, \n",
        "                                                            random_state = 2018)\n",
        "\n",
        "history = model.fit(x = train_images, y = train_masks, batch_size = 16,\n",
        "                          validation_data =(test_images,test_masks), epochs = 20,\n",
        "                          callbacks=my_callbacks)\n",
        "\n",
        "\n",
        "\n",
        "# hist = model.fit(\n",
        "#            X_train[0:10],Y_train[0:10],validation_split=0.1,\n",
        "          \n",
        "#             epochs=my_epochs,\n",
        "#             callbacks=my_callbacks\n",
        "# )\n",
        "        \n",
        "           "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "13/13 [==============================] - 3s 202ms/step - loss: 0.6494 - accuracy: 0.7913 - val_loss: 0.6093 - val_accuracy: 0.9903\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 0.6131 - accuracy: 0.9285 - val_loss: 0.5731 - val_accuracy: 0.9973\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 0.5777 - accuracy: 0.9796 - val_loss: 0.5388 - val_accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.5482 - accuracy: 0.9940 - val_loss: 0.5289 - val_accuracy: 0.9976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "ugd46wWZfhxu",
        "outputId": "8b651897-284b-4f45-ddb0-11ae7c38ab12"
      },
      "source": [
        "#Performing prediction a particular image\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imshow\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "\n",
        "img_path = 'sarcopenia-ai-master/Deblurred_Outputs/deblur2.jpg'\n",
        "img = io.imread(img_path)\n",
        "img1 = resize(img , (256,256,3))\n",
        "\n",
        "prediction = model.predict(img1)\n",
        "\n",
        "plt.imshow(np.squeeze(prediction), cmap = 'ocean')\n",
        "plt.savefig(\"seg3_4.jpg\")\n",
        "plt.axis('off')\n",
        "plt.savefig(\"seg3_a_off.jpg\") #saving the image. You can use a seperate folder to store the images\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}